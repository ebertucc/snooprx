{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "This version holds sites as major frame work, so a drug dictionary is assumed to be information gleaned only from one site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bf\n",
    "import requests\n",
    "import re\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_soup(url):\n",
    "    html = requests.get(url).text\n",
    "    soup = bf(html, 'html5lib')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go get information about drugs in a category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drugs.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each drug has a name and a url_stem\n",
    "class drug:\n",
    "    \n",
    "    def __init__(self):# , name):\n",
    "        self.name = '' #name\n",
    "        self.generic = ''\n",
    "        self.url_drug = '' #{'ddc': '', 'wmd': ''}\n",
    "        self.url_drug_revs = ''\n",
    "        self.reviews = []#{'ddc': '', 'wmd': ''}\n",
    "        self.score = 0     \n",
    "        self.num_rev = 0 #{'ddc': '', 'wmd': ''}\n",
    "        self.num_rev_pages = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new_drug.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****** ambidexterous \n",
    "# for a particular drug\n",
    "# gets urls for pages of reviews\n",
    "def revs_url_list(_new_drug, pg_init, pg_n, site):\n",
    "    if site == 'ddc':\n",
    "        url_c = ['https://www.drugs.com/comments/', '/?page=', '']\n",
    "    elif site == 'wmd':\n",
    "        url_c = ['https://www.webmd.com/drugs/drugreview-', '&pageIndex=', '&sortby=3' ]\n",
    "#     url_list = [url_c[0] + drug_stem + url_c[1] +str(ik) + url_c[2] for ik in range(pg_init, pg_n+1)]\n",
    "    url_list = [ _new_drug.url_drug_revs+ url_c[1] +str(ik) + url_c[2] for ik in range(pg_init, pg_n+1)]\n",
    "#     print(url_list)\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for a particular drug\n",
    "# takes url list, scrapes, returns pages of soup\n",
    "def scraper(url_list):\n",
    "    soup_list = []\n",
    "    for url in url_list:\n",
    "        page = load_soup(url)\n",
    "        soup_list.append( [url, load_soup(url)])\n",
    "    return soup_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for a particular drug\n",
    "# uses methods from the two parser classes to slice and dice soup into review objects\n",
    "# should be a \"site\" method...\n",
    "def parse_reviews(pages, drug, parser, tag):\n",
    "    for page in pages:\n",
    "        rev_stew = page[1].find_all('div' ,{'class' : tag})\n",
    "        for ik, item in enumerate(rev_stew):\n",
    "            new_review = review(drug, item, parser, ik)\n",
    "            drug.reviews.append(new_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a particular drug\n",
    "# Review object\n",
    "class review(drug):\n",
    "    \n",
    "        def __init__(self, drug, _review_soup, site, ik):\n",
    "            \n",
    "            reviewer_info = site.set_reviewerMeta(_review_soup, ik)\n",
    "            self.drugName = drug.name\n",
    "            self.site = site.name\n",
    "            self.condition = site.set_condition(_review_soup)\n",
    "            self.reviewDate = site.set_reviewDate(_review_soup)\n",
    "            self.userName = site.set_userName(reviewer_info) #temp.split(',')[0]\n",
    "            self.ageRange = site.set_ageRange(reviewer_info) #re.search('\\s\\w+[-]\\w+\\s', temp).group().strip()\n",
    "            self.gender = site.set_gender(reviewer_info) #re.split('\\s\\w+[-]\\w+\\s', temp)[1].split()[0]\n",
    "            self.role = site.set_role(_review_soup)\n",
    "            self.medDuration = site.set_medDuration(reviewer_info) #re.split('on Treatment for ', temp)[1].split('(Patient)')[0].strip()\n",
    "            self.effectiveness = site.set_effectiveness(_review_soup)\n",
    "            self.ease_of_use = site.set_ease_of_use(_review_soup)\n",
    "            self.satisfaction = site.set_satisfaction(_review_soup)\n",
    "            self.genRating = site.set_genRating(_review_soup)\n",
    "            self.comment = site.set_comment(_review_soup, ik)\n",
    "            self.upVotes = site.set_upVotes(_review_soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a particular drug\n",
    "# takes url list, scrapes, returns pages of soup\n",
    "def scrape_parse_reviews(url, drug, parser, tag):\n",
    "    page = load_soup(url)\n",
    "    print(len(page))\n",
    "    rev_stew = page.find_all('div' ,{'class' : tag})\n",
    "    for ik, item in enumerate(rev_stew):\n",
    "        new_review = review(drug, item, parser, ik)\n",
    "        drug.reviews.append(new_review)\n",
    "    return drug \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drugs.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************ Drugs.com ************  Parser\n",
    "\n",
    "# modified from source: \n",
    "# https://blog.nycdatascience.com/student-works/web-scraping/anti-epileptic-drug-review-analysis/\n",
    "\n",
    "class DrugsDotCom:\n",
    "    \n",
    "        def __init__(self, name, abbrev):\n",
    "            self.name = name\n",
    "            self.abbrev = abbrev\n",
    "            \n",
    "        def Drugslist_url_list(self, condition, pg_init, pg_n):\n",
    "            url_c = ['https://www.drugs.com/condition/', '.html?page_number=']\n",
    "            url_list = [url_c[0] + condition + url_c[1] +str(ik) for ik in range(pg_init, pg_n+1)]\n",
    "            return url_list\n",
    "        \n",
    "        # function to process soups to extract drug metadata (mix of site specific and standardized) for drugs.com\n",
    "        def process_drug(self, _drug_summary, _drug_condition, site, _new_drug):\n",
    "            name_soup = _drug_summary.find('td', {'class':'condition-table__drug-name' })\n",
    "            _new_drug.name = name_soup.text.strip().split('\\n')[0]\n",
    "            _new_drug.generic = _drug_condition.find('p', {'class': 'condition-table__generic-name'}).text.strip().split('Generic name:\\xa0')[1].strip()\n",
    "            _new_drug.url_drug = 'https://www.drugs.com'+str(name_soup).split('href=\"')[1].split('\" onclick')[0] \n",
    "            _new_drug.num_rev = _drug_summary.find('td', {'class':'condition-table__reviews'}).text.replace('reviews', '').strip()\n",
    "            try:\n",
    "                _new_drug.num_rev= int(_new_drug.num_rev)\n",
    "            except:\n",
    "                _new_drug.num_rev = 0\n",
    "            _new_drug.num_rev_pages = _new_drug.num_rev//25 + 1\n",
    "            _new_drug.score = _drug_summary.find('td', {'class': 'condition-table__rating'}).text.strip()\n",
    "            pop_soup = _drug_summary.find('td', {'class': 'condition-table__popularity'})\n",
    "            popularity = str(pop_soup.find('div', {'class': 'meter'})).split('width:')[1].split('%')[0]\n",
    "            _new_drug.url_drug_revs= 'https://www.drugs.com'+ str(_drug_summary.find('td', {'class': 'condition-table__reviews'})).split('href=\"')[1].split('\"')[0]\n",
    "            return _new_drug\n",
    "            \n",
    "        def get_drug_metadata(self, condition, pg_init, pg_n):\n",
    "            # all drugs for a condition\n",
    "            #call function to build list of all drugs used to treat depression\n",
    "            abbrev = self.abbrev\n",
    "            drugslist_list = self.Drugslist_url_list(condition, pg_init, pg_n)\n",
    "            if abbrev ==  'ddc':\n",
    "                # initialize lists for two kinds of soup needed to fill drug metadata fields for ddc\n",
    "                druglistsummary_soups = []\n",
    "                druglistprofile_soups = []\n",
    "\n",
    "                #for each of the drugs in the list of drugs used to treat the condition (in this case depression), get the two kinds of soup\n",
    "                for url in drugslist_list:\n",
    "                    soup = load_soup(url)\n",
    "                    drug_summary = soup.find_all('tr', {'class': 'condition-table__summary'})\n",
    "                    druglistsummary_soups= druglistsummary_soups+drug_summary\n",
    "                    drug_profile = soup.find_all('tr', {'class': 'condition-profile'})\n",
    "                    druglistprofile_soups = druglistprofile_soups + drug_profile\n",
    "\n",
    "                _drug_list_ds = []\n",
    "                for ik in range(len(druglistsummary_soups)):\n",
    "                    new_drug = drug()\n",
    "                    drug_meta = self.process_drug(druglistsummary_soups[ik], druglistprofile_soups[ik], abbrev, new_drug)\n",
    "                    _drug_list_ds.append(drug_meta)\n",
    "#                     print(drug_meta.__dict__)\n",
    "            return _drug_list_ds\n",
    "        \n",
    "\n",
    "        def get_revs_url_list(self, _new_drug):\n",
    "            cond_soup = load_soup(_new_drug.url_drug_revs)\n",
    "            mega = str(((cond_soup.find('div', {'id': 'contentWrap'})).find('div', {'class': 'contentBox'})))#.find('div', {'class':'responsive-table-wrap-mobile'})\n",
    "            options = mega.split('gotoArr[')[2:-1]\n",
    "            options = [(str(option).split(\"= '\")[1]).split(\"';\\n\")[0] for option in options]\n",
    "            numbers_soup = ((cond_soup.find('div', {'id': 'contentWrap'})).find('div', {'class': 'contentBox'})).find('div', {'class':'data-list-filter'}).find_all('option')[1:]\n",
    "            rev_cts = [ (str(each).split('(')[1]).split(')<')[0] for each in numbers_soup]\n",
    "\n",
    "            revs_urls = []\n",
    "            cond_codes_pgs = []\n",
    "            total_revs = 0\n",
    "            for ik in range(len(options)):\n",
    "                if 'epressi' in options[ik]:\n",
    "                    cond_codes_pgs.append((options[ik], int(rev_cts[ik])))\n",
    "            for cond_pg in cond_codes_pgs:\n",
    "                cond = cond_pg[0]\n",
    "                pg_n = int(cond_pg[1])//25\n",
    "                revs_urls = revs_urls+[\"http://www.drugs.com\"+ cond + '/?page='+ str(ik) for ik in range(1, pg_n+1)]\n",
    "                total_revs +=int(cond_pg[1])\n",
    "            _new_drug.num_rev_pages = total_revs\n",
    "            return _new_drug, revs_urls\n",
    "        \n",
    "        \n",
    "        # fetch information about author;\n",
    "        # bug fix: added a tag to the tags list.  There may be more lurking...\n",
    "        def set_reviewerMeta (self, _rev_soup, ik):\n",
    "            tags = ['user-name user-type user-type-2_non_member', 'user-name user-type user-type-1_standard_member','user-name user-type user-type-0_select_member']\n",
    "            if _rev_soup.find('p', {'class': tags[0]}):\n",
    "                return _rev_soup.find('p', {'class': tags[0]})\n",
    "            elif _rev_soup.find('p', {'class': tags[1]}):\n",
    "                return _rev_soup.find('p', {'class': tags[1]})\n",
    "            elif _rev_soup.find('p', {'class': tags[2]}):\n",
    "                return _rev_soup.find('p', {'class': tags[2]})\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "        def set_userName (self, _reviewerMeta):\n",
    "                try:\n",
    "                    return _reviewerMeta.contents[0].strip()\n",
    "                except:\n",
    "                    return 'Anonymous'\n",
    "\n",
    "                \n",
    "        #need to fix this        \n",
    "        def set_ageRange (self, _reviewerMeta):\n",
    "                try:\n",
    "                    return re.search('\\s\\w+[-]\\w+\\s', _reviewerMeta).group().strip()\n",
    "                except:\n",
    "                    return None\n",
    "                    \n",
    "                    \n",
    "        #gender not specified on drugs.com\n",
    "        def set_gender (self, _reviewerMeta):\n",
    "                return None\n",
    "            \n",
    "        #role not specified on drugs.com\n",
    "        def set_role(self, _rev_soup):\n",
    "            return None\n",
    "            \n",
    "        def set_medDuration (self, _reviewerMeta):\n",
    "                try: \n",
    "                    dates =_reviewerMeta.find_all('span')#, {'class':'small light'})\n",
    "                    if len(dates)>1:\n",
    "                        return str(dates[0]).split('<span class=\"tiny light\">(taken for')[1].split(')</span>')[0].strip()\n",
    "                except:\n",
    "                    return None\n",
    "\n",
    "                \n",
    "        def set_reviewDate (self, _reviewerMeta):\n",
    "                try:\n",
    "                    dates =_reviewerMeta.find_all('span', {'class':\"tiny light comment-date\"})\n",
    "                    if len(dates)>1:\n",
    "                        return str(dates[1]).split('<span class=\"tiny light comment-date\">')[1].split('</span>')[0]\n",
    "                    else:\n",
    "                        return str(dates[0]).split('<span class=\"tiny light comment-date\">')[1].split('</span>')[0]\n",
    "                except:\n",
    "                    return None\n",
    "                \n",
    "                \n",
    "        def set_condition (self, _rev_soup):\n",
    "                try:\n",
    "                    return _rev_soup.find('div', {'class':'user-comment'}).b.get_text()\n",
    "                except:\n",
    "                    return None\n",
    "                \n",
    "                \n",
    "        #from WebMD.com\n",
    "        def set_effectiveness (self, _rev_soup):\n",
    "                return None\n",
    "            \n",
    "            \n",
    "        #from WebMD.com\n",
    "        def set_ease_of_use (self, _rev_soup):\n",
    "                return None\n",
    "            \n",
    "            \n",
    "        #from WebMD.com\n",
    "        def set_satisfaction (self, _rev_soup):\n",
    "                return None\n",
    "\n",
    "            \n",
    "        def set_genRating (self, _rev_soup):        \n",
    "                try:\n",
    "                    return float(_rev_soup.find('div',{'class': 'rating-score'}).get_text())\n",
    "                except:\n",
    "                    return None\n",
    "\n",
    "\n",
    "        def set_comment (self, _rev_soup, ik):\n",
    "                try:\n",
    "                    return _rev_soup.find('div', {'class':'user-comment'}).span.get_text()\n",
    "                except:\n",
    "                    return None\n",
    "\n",
    "\n",
    "        def set_upVotes (self, _rev_soup):\n",
    "                try: \n",
    "                    return int(_rev_soup.find_all('p', {'class':\"tiny light\"})[0].b.get_text().split(' users')[0])\n",
    "                except:\n",
    "                    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************ webMD.com ************ Parser\n",
    "\n",
    "class WebMD:\n",
    "    \n",
    "        def __init__(self, name, abbrev):\n",
    "            self.name = name\n",
    "            self.abbrev = abbrev\n",
    "            \n",
    "        #only one     \n",
    "        def Drugslist_url_list(self, condition, pg_init, pg_n):\n",
    "            url_list = ['https://www.webmd.com/drugs/2/condition-952/major%20depressive%20disorder']#, \n",
    "                        #'https://www.webmd.com/drugs/2/condition-1022/depression', \n",
    "                        #'https://www.webmd.com/drugs/2/condition-13493/depression%20treatment%20adjunct']\n",
    "            return url_list\n",
    "        \n",
    "        # function to process soups to extract drug metadata\n",
    "        def process_drug(self, _drug_summary, site, _new_drug):\n",
    "            drug_data = _drug_summary.find_all('td')\n",
    "            _new_drug.name = drug_data[0].get_text()\n",
    "            _new_drug.url_drug = 'http://www.webmd.com'+str(drug_data[0]).split('a href=\"')[1].split('\">')[0]\n",
    "            name_soup = load_soup(_new_drug.url_drug)\n",
    "            _new_drug.generic = str(name_soup.find('section', {'class':'generic-name'}).find('p')).split('</span>')[1].split('</p>')[0]\n",
    "            _new_drug.num_rev = int((drug_data[3].get_text().split(' Reviews')[0]))\n",
    "            _new_drug.num_rev_pages = _new_drug.num_rev//5 + 1\n",
    "            _new_drug.url_drug_revs=  'http://www.webmd.com'+str(drug_data[3]).split('a href=\"')[1].split('\">')[0]#.split('href=\"')[1].split('\"')[0]\n",
    "#             print(_new_drug.url_drug_revs[site])\n",
    "            return _new_drug\n",
    "            \n",
    "        def get_drug_metadata(self, condition, pg_init, pg_n):\n",
    "            # all drugs for a condition\n",
    "            #call function to build list of all drugs used to treat depression\n",
    "            abbrev = self.abbrev\n",
    "            drugslist_list = self.Drugslist_url_list(condition, pg_init, pg_n)\n",
    "            # initialize lists for two kinds of soup needed to fill drug metadata fields for ddc\n",
    "            druglistsummary_soups = []\n",
    "\n",
    "\n",
    "            #for each of the drugs in the list of drugs used to treat the condition (in this case depression), get the two kinds of soup\n",
    "            for url in drugslist_list:\n",
    "                soup = load_soup(url)\n",
    "                drug_summary = ((soup.find('table', {'class':'drugs-treatments-table'})).find('tbody')).find_all('tr')\n",
    "                druglistsummary_soups= druglistsummary_soups+drug_summary\n",
    "\n",
    "            _drug_list_ds = []\n",
    "            for ik in range(len(druglistsummary_soups)):\n",
    "                new_drug = drug()\n",
    "                drug_meta = self.process_drug(druglistsummary_soups[ik], abbrev, new_drug)\n",
    "                _drug_list_ds.append(drug_meta)\n",
    "#                 print(drug_meta.__dict__)\n",
    "            return _drug_list_ds\n",
    "\n",
    "        def get_revs_url_list(self, _new_drug):\n",
    "            cond_soup = load_soup(_new_drug.url_drug_revs)\n",
    "#             print(_new_drug.url_drug_revs)\n",
    "            cond_codes_pgs= []\n",
    "            options = (cond_soup.find('select', {'id':'conditionFilter'})).find_all('option')\n",
    "            for option in options:\n",
    "                if 'epressi' in option.text:\n",
    "                    cond_codes_pgs.append((str(option).split('value=\"')[1].split('\"')[0], option.text.split('(')[1].split(' reviews')[0]))\n",
    "            revs_urls = []\n",
    "            total_revs = 0\n",
    "            for cond_pg in cond_codes_pgs:\n",
    "                cond = cond_pg[0]\n",
    "                pg_n = int(cond_pg[1])//5\n",
    "#     url_list = [url_c[0] + drug_stem + url_c[1] +str(ik) + url_c[2] for ik in range(pg_init, pg_n+1)]\n",
    "                revs_urls = revs_urls+ [ _new_drug.url_drug_revs+ '&pageIndex=' +str(ik) + '&sortby=3'+'&conditionFilter='+str(cond) for ik in range(0, pg_n+1)]\n",
    "#                 url_list = [ _new_drug.url_drug_revs[site]+ url_c[1] +str(ik) + url_c[2]+url_c[3] for ik in range(pg_init, pg_n+1)]\n",
    "#                 revs_urls = revs_urls+revs_url_list(new_drug, start_num, pg_n, WMD_parser.abbrev, cond)\n",
    "                total_revs +=int(cond_pg[1])\n",
    "            _new_drug.num_rev = total_revs\n",
    "            return _new_drug, revs_urls\n",
    "        \n",
    "            \n",
    "        def set_reviewerMeta (self, _rev_soup, ik):\n",
    "            try:\n",
    "                return _rev_soup.find('p', {'class':'reviewerInfo'}).text.strip('Reviewer: ')\n",
    "            except:\n",
    "                return None\n",
    "            \n",
    "        #below takes reviewer soup\n",
    "        def set_userName (self, _reviewerMeta):\n",
    "            try:\n",
    "                splits = _reviewerMeta.split(',')\n",
    "                if len(splits)>1:\n",
    "                    return splits[0]\n",
    "                else:\n",
    "                    return 'Anonymous'\n",
    "            except:\n",
    "                return 'Anonymous'\n",
    "\n",
    "        def set_ageRange (self, _reviewerMeta):\n",
    "            try:\n",
    "                return re.search('\\s\\w+[-]\\w+\\s', _reviewerMeta).group().strip()\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "        def set_gender (self, _reviewerMeta):\n",
    "            try: \n",
    "                gender = re.split('\\s\\w+[-]\\w+\\s', _reviewerMeta)[1].split()[0]\n",
    "                if gender != 'on':\n",
    "                    return gender \n",
    "            except: \n",
    "                return None\n",
    "        \n",
    "        def set_role(self, _rev_soup):\n",
    "            try:\n",
    "                return _rev_soup.find('p', {'class':'reviewerInfo'}).text.strip('Reviewer: ').split(' ')[-1].replace('(','').replace(')','')\n",
    "            except:\n",
    "                return None\n",
    "            \n",
    "        def set_medDuration (self, _reviewerMeta):\n",
    "            try:\n",
    "                return re.split('on Treatment for ', _reviewerMeta)[1].split('(Patient)')[0].strip()\n",
    "            except:\n",
    "                return None\n",
    "            \n",
    "        #below takes full soup\n",
    "        #untested for webMD\n",
    "        def set_reviewDate  (self, _rev_soup):\n",
    "            try:\n",
    "                return _rev_soup.find('div', {'class': 'date'}).text.split(' ',1)[0]\n",
    "\n",
    "            except:\n",
    "                return None\n",
    "                \n",
    "        def set_condition (self, _rev_soup):\n",
    "            try:\n",
    "                condition = _rev_soup.find('div', {'class': 'conditionInfo'}).text\n",
    "                temp = condition.split('Condition: ')[1]\n",
    "                return temp\n",
    "            except:\n",
    "                return None\n",
    "                \n",
    "        def set_effectiveness (self, _rev_soup):\n",
    "                try:\n",
    "                    temp = _rev_soup.find('div' ,{'class' : 'catRatings firstEl clearfix'}).text\n",
    "                    return float(re.search(r'\\d+', temp).group()) #switch to float\n",
    "                except:\n",
    "                    return None\n",
    "\n",
    "        def set_ease_of_use (self, _rev_soup):\n",
    "                try:\n",
    "                    temp = _rev_soup.find('div' ,{'class' : 'catRatings clearfix'}).text\n",
    "                    return float(re.search(r'\\d+', temp).group())\n",
    "                except:\n",
    "                    return None\n",
    "\n",
    "        def set_satisfaction (self, _rev_soup):\n",
    "                try:\n",
    "                    temp = _rev_soup.find('div' ,{'class' : 'catRatings lastEl clearfix'}).text\n",
    "                    return float(re.search(r'\\d+', temp).group())\n",
    "                except:\n",
    "                    return None\n",
    "                \n",
    "        #from drugs.com        \n",
    "        def set_genRating  (self, _rev_soup):\n",
    "                    return None\n",
    "\n",
    "        def set_comment (self, _rev_soup, ik):\n",
    "                try: \n",
    "                    temp = _rev_soup.find('p', {'id':'comFull'+str(ik+1)}).text\n",
    "                    temp = re.split('Hide Full', temp)[0]\n",
    "                    return temp.lstrip('Comment:')\n",
    "                except:\n",
    "                    return None\n",
    "\n",
    "        def set_upVotes (self, _rev_soup):\n",
    "                try:\n",
    "                    temp = _rev_soup.find('p', {'class' : \"helpful\"}).text\n",
    "                    return int(re.search(r'\\d+', temp).group())\n",
    "                except:\n",
    "                    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#revised\n",
    "# by default will try to load an existing pickle file for existing all_drug_list, and will scrape if load fails or if specified to do so\n",
    "def build_depression_drugs(site, pickleopt, picklename, load_or_scrape='load'):\n",
    "    # Set condition\n",
    "    condition = 'depression'\n",
    "    def doItAll(parser, start_num, tag):\n",
    "        start = time.time()\n",
    "        if load_or_scrape == 'load':\n",
    "            try:\n",
    "                all_drugs_list = pickle.load(open( 'all_drug_list_'+site+'.p', \"rb\" ) )\n",
    "                print('loaded list')\n",
    "            except:\n",
    "                print(\"there isn't a pickle file to work with! I'm going to have to scrape!\")\n",
    "                all_drugs_list = parser.get_drug_metadata('depression', 1, 1)\n",
    "                pickle.dump( all_drugs_list, open( 'all_drug_list_'+site+'.p', \"wb\" ) )\n",
    "                print('scraped list:', time.time()-start)\n",
    "        elif load_or_scrape == 'scrape':\n",
    "            all_drugs_list = parser.get_drug_metadata('depression', 1, 1)\n",
    "            pickle.dump( all_drugs_list, open( 'all_drug_list_'+site+'.p', \"wb\" ) )\n",
    "            print('scraped list:', time.time()-start)\n",
    "            \n",
    "        drug_list = []\n",
    "        generics_list = [new_drug.generic.strip(' systemic') for new_drug in all_drugs_list]\n",
    "        print('number of generics:', len(generics_list))\n",
    "\n",
    "        for new_drug in all_drugs_list[:2]:\n",
    "            if (new_drug.name in generics_list) or (new_drug.num_rev>=200):\n",
    "                print(new_drug.name)\n",
    "                drug_list.append(new_drug)\n",
    "                new_drug, revs_url_list = parser.get_revs_url_list(new_drug)\n",
    "                for ik, url in enumerate(revs_url_list):\n",
    "                    new_drug = scrape_parse_reviews(url, new_drug, parser, tag)\n",
    "                    drug_list[-1] = new_drug\n",
    "                    pickle.dump( drug_list, open( picklename+'.p', \"wb\" ) )\n",
    "                    print('scraped pages:', time.time()- start, new_drug.name, 'pages:', ik)\n",
    "                    if ik == 1:\n",
    "                        break\n",
    "                print('scraped sites:', time.time()- start)\n",
    "                print('number of drugs on short list so far:', len(drug_list))\n",
    "        return drug_list, all_drugs_list, generics_list\n",
    "\n",
    "    if site == 'ddc':\n",
    "        # Initialize site objects\n",
    "        DDC_parser = DrugsDotCom('Drugs_dot_com', 'ddc')\n",
    "        tag = 'block-wrap comment-wrap'\n",
    "        filled_drug_list, all_drugs_list, generics_list = doItAll(DDC_parser,0, tag)\n",
    "        \n",
    "    elif site == 'wmd':\n",
    "        WMD_parser = WebMD('WebMD', 'wmd')\n",
    "        tag = 'userPost'\n",
    "        filled_drug_list, all_drugs_list, generics_list = doItAll(WMD_parser,1, tag)\n",
    "        \n",
    "    return filled_drug_list, all_drugs_list, generics_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded list\n",
      "number of generics: 25\n",
      "Cymbalta\n",
      "2\n",
      "scraped pages: 0.8740384578704834 Cymbalta pages: 0\n",
      "2\n",
      "scraped pages: 1.3190727233886719 Cymbalta pages: 1\n",
      "scraped sites: 1.3195428848266602\n",
      "number of drugs on short list so far: 1\n",
      "Zoloft\n",
      "2\n",
      "scraped pages: 2.0468664169311523 Zoloft pages: 0\n",
      "2\n",
      "scraped pages: 2.4867868423461914 Zoloft pages: 1\n",
      "scraped sites: 2.4867868423461914\n",
      "number of drugs on short list so far: 2\n"
     ]
    }
   ],
   "source": [
    "filled_drugs_ddc, all_drugs_ddc, generics_list_ddc  = build_depression_drugs('ddc', 'y', 'drug_list_ddc_mini2', 'load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# troubleshoot soup\n",
    "page = load_soup('http://www.drugs.com/comments/duloxetine/cymbalta-for-depression.html/?page=1')\n",
    "tag = 'block-wrap comment-wrap'\n",
    "rev_stew = page.find_all('div' ,{'class' : tag})\n",
    "_rev_soup = rev_stew[0]\n",
    "float(_rev_soup.find('div',{'class': 'rating-score'}).get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check \n",
    "mini_drug_list = pickle.load(open('drug_list_ddc_mini2.p', 'rb'))\n",
    "mini_drug_list[0].reviews[5].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump( filled_drugs_ddc, open( \"drug_list_ddc2.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drug_list_ddc2 = drug_list_ddc\n",
    "del drug_list_ddc, new_drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drug_list_ddc = pickle.load( open(\"drug_list_ddc.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WebMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded list\n",
      "number of generics: 81\n",
      "Cymbalta\n",
      "2\n",
      "scraped pages: 1.5092878341674805 Cymbalta pages: 0\n",
      "2\n",
      "scraped pages: 2.297212600708008 Cymbalta pages: 1\n",
      "scraped sites: 2.2977135181427\n",
      "number of drugs on short list so far: 1\n",
      "Lexapro\n",
      "2\n",
      "scraped pages: 3.8389458656311035 Lexapro pages: 0\n",
      "2\n",
      "scraped pages: 4.8069541454315186 Lexapro pages: 1\n",
      "scraped sites: 4.807432651519775\n",
      "number of drugs on short list so far: 2\n"
     ]
    }
   ],
   "source": [
    "drug_list_wmd, all_drugs_wmd, generics_list_md = build_depression_drugs('wmd', 'n', 'drug_list_wmd_mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ageRange': '35-44',\n",
       " 'comment': 'I was on Cymblata for roughly 2.5 years. At first it helped quite a bit helpful with my depression. It was initially not helpful for my anxiety. \\n\\nSlowly it became less effective. Increasing the dose made my anxiety worse. \\n\\nIn between doses the withdrawal would start to begin: brain shocks, swishing head, dissociation, panic, brain crunch, confusion, muscle pain, and body buzzing. If I missed a dose, I literally would think I was in some hellish nightmare. \\n\\nI began to feel as if Cymbalta was poisoning me.\\n\\nBy the lucky chance (experience of a friend), I found I could switch to Celexa. In combination with fish oil, the Cymbalta withdrawal has been minor. My mood and clarity of mind have been getting better daily. My anxiety is also going down.  ',\n",
       " 'condition': 'Major Depressive Disorder',\n",
       " 'drugName': 'Cymbalta',\n",
       " 'ease_of_use': 3.0,\n",
       " 'effectiveness': 2.0,\n",
       " 'genRating': None,\n",
       " 'gender': 'Male',\n",
       " 'medDuration': '2 to less than 5 years',\n",
       " 'reviewDate': '8/18/2017',\n",
       " 'role': 'Patient',\n",
       " 'satisfaction': 1.0,\n",
       " 'site': 'WebMD',\n",
       " 'upVotes': 14,\n",
       " 'userName': 'Eldo'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check \n",
    "mini_drug_list = pickle.load(open('drug_list_wmd_mini.p', 'rb'))\n",
    "mini_drug_list[0].reviews[1].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(drug_list_wmd, open(\"drug_list_wmd.p\", 'wb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddc_names = [(drug.name).lower() for drug in drug_list_ddc]\n",
    "wmd_names = [(drug.name).lower() for drug in drug_list_wmd]\n",
    "\n",
    "crossover = []\n",
    "for drug_ddc in ddc_names:\n",
    "    if drug_ddc in wmd_names:\n",
    "        crossover.append(drug_ddc)\n",
    "        \n",
    "print(crossover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Counter({'[1, 6]': 1, '[24, 60]': 1, '1 to less than 2 years': 1, '[0, 1]': 1, '[60, 120]': 1})\n",
      "Counter({'[65, 74]': 2, '[55, 64]': 2, 'None': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "rev_list = []\n",
    "age_list = []\n",
    "\n",
    "reviews = wmd_dicts['Abilify'].reviews['wmd']\n",
    "\n",
    "for review in reviews:\n",
    "    rev_list.append(str(review.medDuration))\n",
    "    age_list.append(str(review.ageRange))\n",
    "\n",
    "print(type(rev_list[0][0]))\n",
    "print(Counter(rev_list))\n",
    "print(Counter(age_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2 = enchant.DictWithPWL(\"en_US\",\"new_words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2.check('wikkipedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drug_list = ['Cymbalta', 'Zoloft', 'Lexapro', 'Prozac', 'Celexa', 'bupropion', 'citalopram', 'Wellbutrin XL', 'sertraline', 'Abilify', 'fluoxetine', 'Xanax', 'Wellbutrin', 'Effexor XR', 'venlafaxine', 'Pristiq', 'Effexor', 'escitalopram', 'Paxil', 'trazodone', 'Wellbutrin SR', 'mirtazapine', 'Remeron', 'Viibryd', 'duloxetine', 'paroxetine', 'Seroquel XR', 'amitriptyline', 'alprazolam', 'Deplin', 'lamotrigine', 'Zyprexa', 'nortriptyline', 'quetiapine', 'aripiprazole', 'Desyrel', 'Fetzima', 'tramadol', 'Alprazolam Intensol', 'Budeprion XL', 'Niravam', 'Oleptro', 'Trintellix', 'Aplenzin', 'Budeprion SR', 'Desyrel Dividose', 'doxepin', 'Forfivo XL', 'olanzapine', 'Paxil CR', 'Prozac Weekly', 'risperidone', 'Serzone', 'imipramine', 'lithium', 'Rapiflux', 'Asendin', 'desipramine', 'desvenlafaxine', 'Emsam', 'Irenka', 'l-methylfolate', 'Methylin ER', 'methylphenidate', 'Nardil', 'nefazodone', 'niacin', 'Pamelor', 'Parnate', 'Pexeva', 'Remeron SolTab', 'Sinequan', 'Symbyax', 'vortioxetine', 'amitriptyline', 'chlordiazepoxide', 'amoxapine', 'Anafranil', 'Aventyl Hydrochloride', 'clomipramine', 'fluoxetine / olanzapine', 'fluvoxamine', 'Limbitrol', 'Limbitrol DS', 'modafinil', 'Norpramin', 'paliperidone', 'Rexulti', 'selegiline', 'Tofranil', 'vilazodone', 'Zyprexa Zydis', 'amitriptyline','perphenazine', 'armodafinil', 'atomoxetine', 'brexpiprazole', 'Duo-Vil', 'Etrafon', 'Etrafon Forte', 'isocarboxazid', 'Khedezla', 'L-Methylfolate Formula', 'L-Methylfolate Forte', 'levomilnacipran', 'lisdexamfetamine', 'Ludiomil', 'maprotiline', 'Marplan', 'phenelzine', 'protriptyline', 'Surmontil', 'Tofranil-PM', 'tranylcypromine', 'Triavil', 'trimipramine', 'Vivactil', 'XaQuil XR']\n",
    "drug_list_all = ['hctz','ssri','snri','xr','suboxone','respirdal', 'meth','geodon','benztropine', 'valium','lyrica','melatonin','lamictal','depakote','cogentin','neurontin','nexium','hydralazine','topamax', 'ambien','provigil', 'mirapex', 'saphris','miralax','zolpidem', 'Percocet', 'adderall','risperdal','buspirone', 'lorazepam', 'ativan', 'lunesta','vistaril', 'Strattera','Clonazepam','Savella' , 'Pamelor', 'Paxil CR', 'Endep tablet', 'Irenka', 'Trintellix', 'Serzone', 'Remeron', 'clomipramine HCL', 'Viibryd', 'fluvoxamine', 'Pexeva', 'Desyrel', 'nefazodone', 'Norfranil tablet', 'Limbitrol', 'Elavil Solution', 'fluvoxamine MALEATE ER', 'Oleptro', 'doxepin HCL', 'nortriptyline', 'L-Methylfolate Forte', 'imipramine', 'Alprazolam Intensol', 'trazodone', 'vortioxetine', 'Seroquel','Ludiomil', 'escitalopram', 'Emsam', 'Etnofril tablet', 'Limbitrol DS', 'Forfivo XL', 'L-Methylfolate Formula', 'Asendin tablet', 'doxepin', 'Khedezla', 'trimipramine MALEATE', 'Budeprion SR', 'Fetzima', 'Rexulti', 'risperidone', 'protriptyline', 'E-Vill 50 tablet', 'Symbyax', 'Nardil', 'Sinequan Concentrate', 'desipramine', 'Deconil tablet', 'Triavil', 'citalopram', 'Norpramin', 'desipramine HCL', 'E-Vill 100 tablet', 'paliperidone', 'duloxetine', 'Sk-Pramine tablet', 'Budeprion XL', 'Amitid tablet', 'E-Vill 25 tablet', 'bupropion', 'venlafaxine', 'Tofranil', 'Sinequan', 'Parnate', 'Vivactil', 'isocarboxazid', 'amitriptyline HCL', 'Adapin capsule', 'atomoxetine', 'doxepin HCL capsule', 'alprazolam', 'desvenlafaxine', 'Anafranil', 'lamotrigine', 'tramadol', 'Janimine tablet', 'imipramine HCl', 'lisdexamfetamine', 'Rapiflux', 'Stabanil tablet', 'Paxil', 'aripiprazole', 'Aplenzin', 'amitriptyline / chlordiazepoxide', 'Remeron SolTab', 'sertraline', 'Aventyl Hydrochloride', 'Aventyl capsule', 'Prozac Weekly', 'l-methylfolate', 'Elavil tablet', 'Lexapro', 'Duo-Vil', 'Niravam', 'phenelzine SULFATE', 'armodafinil', 'trimipramine', 'amoxapine', 'Surmontil', 'amitriptyline Solution', 'amitriptyline', 'Effexor XR', 'Methylin ER', 'Vanatrip tablet', 'quetiapine', 'selegiline', 'amitriptyline / perphenazine', 'phenelzine', 'Wellbutrin XL', 'E-Vill 75 tablet', 'Tofranil-PM', 'Emitrip tablet', 'Enovil Solution', 'levomilnacipran', 'XaQuil XR', 'vilazodone', 'Luvox CR', 'Effexor', 'brexpiprazole', 'Zoloft', 'imipramine pamoate', 'protriptyline HCL', 'Etrafon', 'Luvox tablet', 'niacin', 'thyroid desiccated', 'Xanax', 'maprotiline', 'Pristiq', 'E-Vill 10 tablet', 'Wellbutrin', 'doxepin tablet', 'Celexa', 'Sinequan capsule', 'Abilify', 'Deplin', 'maprotiline HCL', 'Marplan', 'Imavate tablet', 'tranylcypromine', 'lithium', 'Ludiomil tablet', 'paroxetine', 'Wellbutrin SR', 'olanzapine', 'Prozac', 'modafinil', 'Kenvil tablet', 'Zyprexa Zydis', 'Zyprexa', 'Cymbalta', 'Re-Live tablet', 'fluvoxamine MALEATE', 'Asendin', 'clomipramine', 'Q.E.L tablet', 'methylphenidate', 'fluoxetine', 'Desyrel Dividose', 'Etrafon Forte', 'Seroquel XR', 'fluoxetine / olanzapine', 'mirtazapine', 'nortriptyline HCL']\n",
    "drug_list = [word.lower() for word in drug_list_all]\n",
    "# if we need more drug names... https://druginfo.nlm.nih.gov/drugportal/drug/names\n",
    "\n",
    "adds = ['weepiness','dr', 'meds','mdd','aspergers','crittenden','walmart','wikkipedia', 'dysthymia','ocd','apnea','ptsd', 'occ','pancreatitis','electroconvulsive','neuropathy','mgs','antipsychotic','thyrotoxicosis','oculogyric','ADHD','pychotic','tricyclic','mitral', 'dyskinesia', 'hypomanic','hypomania', 'dystonic', 'tardive', 'Pristq', 'fibromyalgia', 'akathesia' ]\n",
    "adds = [word.lower() for word in adds]\n",
    "\n",
    "new_words = drug_list+adds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_list = []\n",
    "error_ref = []\n",
    "\n",
    "chkr = SpellChecker(\"en_US\", filters=[EmailFilter,URLFilter])\n",
    "\n",
    "def spellchecker(reviews):\n",
    "    for ik, review in enumerate(reviews):\n",
    "    #         print ('-------------')\n",
    "    #         print (review['comment'])\n",
    "    #         print(ftfy.fix_text(review['comment'])#['review'])\n",
    "    #         words = [word.lower() for word in words]\n",
    "            chkr.set_text(review.comment)\n",
    "            for err in chkr:\n",
    "                error = (err.word).lower()\n",
    "                options = [change.lower() for change in d2.suggest(error)]\n",
    "                if error in options:\n",
    "                    continue\n",
    "                else:\n",
    "                    catch = 'no'\n",
    "                    word_to_beat = options[0]\n",
    "                    score_to_beat = 10\n",
    "                    for word in options:\n",
    "                        dist = nltk.edit_distance(error, word)\n",
    "                        if (word in new_words):# and (nltk.edit_distance(error, word)<score_to_beat):\n",
    "                            if (nltk.edit_distance(error, word)<=score_to_beat):\n",
    "                                word_to_beat = word\n",
    "                                score_to_beat = nltk.edit_distance(error, word)\n",
    "                                catch = 'yes'\n",
    "                        elif dist<score_to_beat:\n",
    "                            word_to_beat = word\n",
    "                            score_to_beat = nltk.edit_distance(error, word)\n",
    "                    print('final option:', error, word_to_beat, score_to_beat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final option: dont dot 1\n",
      "final option: hurrah hurray 1\n"
     ]
    }
   ],
   "source": [
    "spellchecker(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two test cases below:\n",
    "    1.) Abilify with WebMD\n",
    "    2.) Methylphenidate with Drugs.com\n",
    "    \n",
    "the code uses url_stem and range of pages to generate a url list, then goes and scrapes those pages,\n",
    "and parses them into reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WebMD Abilify test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# retired\n",
    "# builds url lists specific to the site\n",
    "def WebMD_url_list(drug_stem, pg_init, pg_n):\n",
    "    url_c = ['https://www.webmd.com/drugs/drugreview-', '&pageIndex=', '&sortby=3&conditionFilter=-500' ]\n",
    "    url_list = [url_c[0] + drug_stem + url_c[1] +str(ik) + url_c[2] for ik in range(pg_init, pg_n+1)]\n",
    "    return url_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Abilify Soup\n",
    "abilify_stem = '64439-Abilify-oral.aspx?drugid=64439&drugname=Abilify-oral'\n",
    "abilify = drug('abilify', abilify_stem)\n",
    "abilify_Soup = scraper(WebMD_url_list(abilify.url_stem, 0, 140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse Abilify Soup\n",
    "abilify = drug('abilify', abilify_stem)\n",
    "webMD_parser = WebMD('webMD')\n",
    "webMD_tag = 'userPost'\n",
    "parse_reviews(abilify_Soup, webMD_tag, abilify, webMD_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prints review objects for inspection\n",
    "\n",
    "for reviewx in abilify.reviews[0:10]:\n",
    "    for key in reviewx.__dict__:\n",
    "        print(key, ':', reviewx.__dict__[key])\n",
    "    print('----------')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trouble shoot patient metadata\n",
    "abilify_Soup[0][1].find('p', {'class':'reviewerInfo'}).text\n",
    "rev_stew[1].find('p', {'class':'reviewerInfo'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for page in abilify_Soup:\n",
    "        rev_stew = page[1].find_all('div' ,{'class' : webMD_tag})\n",
    "        for ik, item in enumerate(rev_stew):\n",
    "            print(item)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drugs.com Meth test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drugs.com test\n",
    "\n",
    "meth_stem = 'methylphenidate'\n",
    "meth = drug('methylphenidate', meth_stem)\n",
    "meth_Soup = scraper(Drugs_url_list(meth.url_stem, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meth = drug('methylphenidate', meth_stem)\n",
    "drugs_tag = 'block-wrap comment-wrap'\n",
    "drugs_parser = DrugsDotCom('drugsDotCom')\n",
    "parse_reviews(meth_Soup, drugs_tag, meth, drugs_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle Soup (at the very least--work on pickling objects (?) later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_list = []\n",
    "for reviewx in abilify.reviews:\n",
    "    dict_list.append(reviewx.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump( dict_list, open( \"abilify.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side project: scraping drug list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# url = 'https://www.drugs.com/condition/depression.html'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Drugslist_url_list(condition, pg_init, pg_n):\n",
    "    url_c = ['https://www.drugs.com/condition/', '.html?page_number=']\n",
    "    url_list = [url_c[0] + condition + url_c[1] +str(ik) for ik in range(pg_init, pg_n+1)]\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drugslist_list = Drugslist_url_list('depression', 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "druglistsummary_soups = []\n",
    "druglistprofile_soups = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for url in drugslist_list:\n",
    "    soup = load_soup(url)\n",
    "    drug_summary = soup.find_all('tr', {'class': 'condition-table__summary'})\n",
    "    druglistsummary_soups= druglistsummary_soups+drug_summary\n",
    "    drug_profile = soup.find_all('tr', {'class': 'condition-profile'})\n",
    "    druglistprofile_soups = druglistprofile_soups + drug_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drug_summary[-5].find('td', {'class':'condition-table__drug-name' }).text.strip().split('\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_drug(_drug_summary, _drug_condition):\n",
    "    name_soup = _drug_summary.find('td', {'class':'condition-table__drug-name' })\n",
    "    name = name_soup.text.strip().split('\\n')[0]\n",
    "    url_drug = 'https://www.drugs.com'+str(name_soup).split('href=\"')[1].split('\" onclick')[0]\n",
    "    num_rev = _drug_summary.find('td', {'class':'condition-table__reviews'}).text.replace('reviews', '').strip()\n",
    "    score = _drug_summary.find('td', {'class': 'condition-table__rating'}).text.strip()\n",
    "    pop_soup = _drug_summary.find('td', {'class': 'condition-table__popularity'})\n",
    "    popularity = str(pop_soup.find('div', {'class': 'meter'})).split('width:')[1].split('%')[0]\n",
    "    generic = _drug_condition.find('p', {'class': 'condition-table__generic-name'}).text.strip().split('Generic name:\\xa0')[1].strip()\n",
    "    drug_comments_url = drug_soup.find('div', {'id': 'content-box-nav-tabs'})\n",
    "    lis = drug_comments_url.find_all('li')\n",
    "    for li in lis:\n",
    "        if li.text == 'User Reviews':\n",
    "            url_drug_rev= 'https://www.drugs.com'+ r(li).split('href=\"')[1].split('\" onclick')[0]\n",
    "    drug_d = {'name':name, 'generic': generic, 'url_drug': url_drug, 'url_drug_rev': url_drug_rev, 'num_rev':num_rev, 'score':score, 'popularity': popularity}\n",
    "    return drug_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drug_list_ds = []\n",
    "for ik in range(len(druglistmeta_soups)):\n",
    "    drug_meta = process_drug(druglistsummary_soups[ik], druglistprofile_soups[ik])\n",
    "#     drug_soup = load_soup(drug_meta['url_drug'])\n",
    "    print(drug_meta['name'], '(', drug_meta['generic'], ')')\n",
    "    drug_list_ds.append(drug_meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this list reflects the list of all user initiated categories on drugs.com for depression\n",
    "# the next question is: which should be merged because the generic and brand are discussed seperately\n",
    "for ik in range(len(drug_list_ds)):\n",
    "    print(drug_list_ds[ik]['name'],': ', drug_list_ds[ik]['num_rev'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging (or not) generics and brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now unnecessary\n",
    "def Drugs_url_list(drug_stem, pg_init, pg_n):\n",
    "    url_c = ['https://www.drugs.com/comments/', '/?page=']\n",
    "    url_list = [url_c[0] + drug_stem + url_c[1] +str(ik) for ik in range(pg_init, pg_n+1)]\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pulling reviews url from drug info page\n",
    "drug_soup = load_soup(drug_list_ds[0]['url_drug'])\n",
    "drug_comments_url = drug_soup.find('div', {'id': 'content-box-nav-tabs'})\n",
    "lis = drug_comments_url.find_all('li')\n",
    "for li in lis:\n",
    "    if li.text == 'User Reviews':\n",
    "        rev_sffx = str(li).split('href=\"')[1].split('\" onclick')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generic = \n",
    "(drug_soup.find('p', {'class': 'drug-subtitle'})).text.split(\"Generic Name: \")[1].split(' ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#brand_names = \n",
    "(drug_soup.find('p', {'class': 'drug-subtitle'})).text.split(\"Generic Name: \")[1].split(')')[1].split('Brand Names: ')[1].split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#webmd process each drug meta data from drug list\n",
    "def process_drug(_drug_summary, _drug_condition, site, _new_drug):\n",
    "    drug_data = _drug_summary.find_all('td')\n",
    "    _new_drug.name = drug_data[0].get_text()\n",
    "    _new_drug.url_drug[site] = 'http://www.webmd.com/'+str(drug_data[0]).split('a href=\"')[1].split('\">')[0]\n",
    "    name_soup = load_soup(drug_url)\n",
    "    _new_drug.generic = str(name_soup.find('section', {'class':'generic-name'}).find('p')).split('</span>')[1].split('</p>')[0]\n",
    "    _new_drug.num_rev[site] = (drug_data[3].get_text().split(' Reviews')[0])\n",
    "    _new_drug.url_drug_revs[site]=  'http://www.webmd.com/'+str(drug_data[3]).split('a href=\"')[1].split('\">')[0].split('href=\"')[1].split('\"')[0]\n",
    "    return _new_drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#webmd hasn't been written \n",
    "# fyi: https://www.webmd.com/depression/guide/depression-medications-antidepressants\n",
    "\n",
    "depression_soup = load_soup('https://www.webmd.com/drugs/2/condition-1022/depression')\n",
    "drug_list = ((depression_soup.find('table', {'class':'drugs-treatments-table'})).find('tbody')).find_all('tr')\n",
    "\n",
    "for drug in drug_list:\n",
    "    drug_data = drug.find_all('td')\n",
    "    name = drug_data[0].get_text()\n",
    "    drug_url = 'http://www.webmd.com/'+str(drug_data[0]).split('a href=\"')[1].split('\">')[0]\n",
    "    name_soup = load_soup(drug_url)\n",
    "    num_revs = (drug_data[3].get_text().split(' Reviews')[0])\n",
    "    drug_revs_url = str(drug_data[3]).split('a href=\"')[1].split('\">')[0]\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
